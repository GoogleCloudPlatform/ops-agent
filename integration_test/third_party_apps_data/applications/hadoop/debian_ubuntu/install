set -e

sudo apt-get update
sudo DEBIAN_FRONTEND=noninteractive apt-get install -y default-jdk default-jdk-headless wget

sudo mkdir -p \
    /opt/hadoop/logs \
    /opt/volume/datanode \
    /opt/volume/namenode

wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz
sudo tar -xf hadoop-3.3.1.tar.gz -C /opt/hadoop --strip 1

JAVA_HOME="$(dirname "$(dirname "$(readlink "$(readlink "$(which javac)")")")")"

cat <<EOF | sudo tee /opt/hadoop/env
export JAVA_HOME=$JAVA_HOME
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
export HADOOP_HOME=/opt/hadoop
export HADOOP_COMMON_HOME=/opt/hadoop
export HADOOP_HDFS_HOME=/opt/hadoop
export HADOOP_MAPRED_HOME=/opt/hadoop
export HADOOP_YARN_HOME=/opt/hadoop
export HADOOP_OPTS="-Djava.library.path=/opt/hadoop/lib/native"
export HADOOP_COMMON_LIB_NATIVE_DIR=/opt/hadoop/lib/native
export PATH=$PATH:/opt/hadoop/sbin:/opt/hadoop/bin
EOF
source /opt/hadoop/env

cat <<EOF | sudo tee /etc/systemd/system/hadoop.service
[Unit]
Description=Hadoop database
After=network.target

[Service]
Type=forking

Environment=JAVA_HOME=$JAVA_HOME
Environment=PATH=$PATH:$JAVA_HOME/bin
Environment=CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
Environment=HADOOP_HOME=$HADOOP_HOME
Environment=HADOOP_COMMON_HOME=$HADOOP_COMMON_HOME
Environment=HADOOP_HDFS_HOME=$HADOOP_HDFS_HOME
Environment=HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME
Environment=HADOOP_YARN_HOME=$HADOOP_YARN_HOME
Environment=HADOOP_OPTS="$HADOOP_OPTS"
Environment=HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_COMMON_LIB_NATIVE_DIR
Environment=PATH=$PATH:/opt/hadoop/sbin:/opt/hadoop/bin
ExecStart=/opt/hadoop/sbin/start-dfs.sh
WorkingDirectory=/opt/hadoop
User=hadoop
Group=hadoop
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

cat <<EOF | sudo tee /opt/hadoop/etc/hadoop/core-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000/</value>
</property>
</configuration>
EOF

cat <<EOF | sudo tee /opt/hadoop/etc/hadoop/hdfs-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
    <name>dfs.data.dir</name>
    <value>file:///opt/volume/datanode</value>
  </property>
  <property>
    <name>dfs.name.dir</name>
    <value>file:///opt/volume/namenode</value>
</property>
</configuration>
EOF

cat <<EOF | sudo tee /opt/hadoop/etc/hadoop/hadoop-env.sh
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

export JAVA_HOME=\${JAVA_HOME}
export HADOOP_CONF_DIR=\${HADOOP_CONF_DIR:-"/etc/hadoop"}
for f in \$HADOOP_HOME/contrib/capacity-scheduler/*.jar; do
  if [ "\$HADOOP_CLASSPATH" ]; then
    export HADOOP_CLASSPATH=\$HADOOP_CLASSPATH:\$f
  else
    export HADOOP_CLASSPATH=\$f
  fi
done
export HADOOP_OPTS="\$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=\${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=\${HDFS_AUDIT_LOGGER:-INFO,NullAppender} \$HADOOP_NAMENODE_OPTS"
export HADOOP_NAMENODE_OPTS="-Dcom.sun.management.jmxremote \$HADOOP_NAMENODE_OPTS"
export HADOOP_NAMENODE_OPTS="\$HADOOP_NAMENODE_OPTS -Dcom.sun.management.jmxremote.authenticate=false"
export HADOOP_NAMENODE_OPTS="\$HADOOP_NAMENODE_OPTS -Dcom.sun.management.jmxremote.ssl=false"
export HADOOP_NAMENODE_OPTS="\$HADOOP_NAMENODE_OPTS -Dcom.sun.management.jmxremote.port=8004 -Dcom.sun.management.jmxremote.rmi.port=8004"
export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS \$HADOOP_DATANODE_OPTS"
export HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=\${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=\${HDFS_AUDIT_LOGGER:-INFO,NullAppender} \$HADOOP_SECONDARYNAMENODE_OPTS"
export HADOOP_NFS3_OPTS="\$HADOOP_NFS3_OPTS"
export HADOOP_PORTMAP_OPTS="-Xmx512m \$HADOOP_PORTMAP_OPTS"
export HADOOP_CLIENT_OPTS="\$HADOOP_CLIENT_OPTS"
if [ "\$HADOOP_HEAPSIZE" = "" ]; then
  export HADOOP_CLIENT_OPTS="-Xmx512m \$HADOOP_CLIENT_OPTS"
fi
export HADOOP_SECURE_DN_USER=\${HADOOP_SECURE_DN_USER}
export HADOOP_PID_DIR=\${HADOOP_PID_DIR}
export HADOOP_SECURE_DN_PID_DIR=\${HADOOP_PID_DIR}
export HADOOP_IDENT_STRING=\$USER
EOF

getent group hadoop || sudo groupadd hadoop
id -u hadoop || sudo useradd -s /bin/bash -d /opt/hadoop hadoop -g hadoop

sudo chown -R hadoop:hadoop /opt/hadoop
sudo chown -R hadoop:hadoop /opt/volume

sudo -u hadoop bash -c "cat /opt/hadoop/env >> ~/.bashrc"
sudo -u hadoop bash -c "echo -e 'y\n' | ssh-keygen -b 2048 -t rsa -f ~/.ssh/id_rsa -q -N \"\""
sudo -u hadoop bash -c "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys"

sudo -u hadoop bash -c "source /opt/hadoop/env && hdfs namenode -format"

sudo systemctl daemon-reload
sudo systemctl enable hadoop
sudo systemctl restart hadoop
sleep 60
